# HelpingHand
<!-- ALL-CONTRIBUTORS-BADGE:START - Do not remove or modify this section -->
[![All Contributors](https://img.shields.io/badge/all_contributors-1-orange.svg?style=flat-square)](#contributors-)
<!-- ALL-CONTRIBUTORS-BADGE:END -->

[![forthebadge](https://forthebadge.com/images/badges/built-by-developers.svg)](https://forthebadge.com)
[![forthebadge](https://forthebadge.com/images/badges/built-for-android.svg)](https://forthebadge.com)
[![forthebadge](https://forthebadge.com/images/badges/powered-by-responsibility.svg)](https://forthebadge.com)
[![forthebadge](https://forthebadge.com/images/badges/open-source.svg)](https://forthebadge.com)
[![forthebadge](https://forthebadge.com/images/badges/made-with-reason.svg)](https://forthebadge.com)

## üìå Introduction

Even with the rise of tools and technologies, mankind hasn‚Äôt implemented applications that could help visually impaired people. With the rise of Data Modelling techniques that can be used to infuse ‚Äúintelligence‚Äù even in dumb computers and the ease of accessibility, this ‚Äúintelligence‚Äù can be extended to our Smartphone to help the visually impaired people cope up with their surroundings and get a helping hand in their daily activities. Our Application aims to bridge the gap between them and the visual world by leveraging the power of Deep Learning which can be made accessible even on low-ended devices with a lucid User-Interface that would exactly allow them to better understand the world around.

## üèÅ Technology Stack

* [Flutter](https://flutter.dev/)
* [Tensorflow](https://www.tensorflow.org/)
* [Keras](https://keras.io/)
* [Flask](https://flask.palletsprojects.com/)
* [Firebase](https://firebase.google.com/)

## üèÉ‚Äç‚ôÇÔ∏è Why this Project?

Our primary purpose behind this project is to leverage and study how Deep Learning Architectures along with easy prototyping tools can help us develop applications that can be easily rendered even on low-end devices. With this Application, we will develop a one-stop solution to allow the Blind or Partially Blind People to better understand the surroundings around them and to be able to cope with the dynamic world ahead of them. 

The Minimal Viable Product (MVP) would allow the Users to leverage Image Captioning Architecture to generate a real-time insight into their surroundings while using Natural Language Processing to speak out in a lucid manner. The cornerstone of the Application would be its User Interface which would infuse a lucid experience for the User with its ease of handling and use.

For this project, we will be collaborating on various domains like: 
- Data Modelling
- RESTful API Development
- Prototyping Mobile Application using Flutter 
- UI/UX Designing

This would be an enriching experience for all of us that are part of this team.

## üí• Contributors

- [Yash Khare](https://github.com/yashk2000)
- [Shambhavi Aggarwal](https://github.com/agg-shambhavi)
- [Harsh Mishra](https://github.com/HarshCasper)

## üìú LICENSE

[MIT](https://github.com/HarshCasper/HelpingHand/blob/master/LICENSE)

## Contributors ‚ú®

Thanks goes to these wonderful people ([emoji key](https://allcontributors.org/docs/en/emoji-key)):

<!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section -->
<!-- prettier-ignore-start -->
<!-- markdownlint-disable -->
<table>
  <tr>
    <td align="center"><a href="https://www.linkedin.com/in/shambhavi-aggarwal-437804179/"><img src="https://avatars0.githubusercontent.com/u/48705124?v=4" width="100px;" alt=""/><br /><sub><b>Shambhavi Aggarwal</b></sub></a><br /><a href="https://github.com/HarshCasper/HelpingHand/commits?author=agg-shambhavi" title="Code">üíª</a></td>
  </tr>
</table>

<!-- markdownlint-enable -->
<!-- prettier-ignore-end -->
<!-- ALL-CONTRIBUTORS-LIST:END -->

This project follows the [all-contributors](https://github.com/all-contributors/all-contributors) specification. Contributions of any kind welcome!